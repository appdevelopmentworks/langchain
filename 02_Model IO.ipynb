{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95075c49-04fa-49eb-b83a-777f4ed52910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-9L6q6x7HCDtqe8gjBEvhHh8wAiDQQ\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1714818206,\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"\\u9b5a\\u306f\\u57fa\\u672c\\u7684\\u306b\\u306f\\u6c34\\u4e2d\\u3067\\u305a\\u3063\\u3068\\u6cf3\\u304e\\u7d9a\\u3051\\u3066\\u3044\\u308b\\u305f\\u3081\\u3001\\u7720\\u308b\\u3068\\u3044\\u3046\\u6982\\u5ff5\\u304c\\u3042\\u307e\\u308a\\u3042\\u308a\\u307e\\u305b\\u3093\\u3002\\u305f\\u3060\\u3057\\u3001\\u4e00\\u90e8\\u306e\\u9b5a\\u985e\\u306f\\u4f11\\u606f\\u306e\\u305f\\u3081\\u306b\\u4f53\\u3092\\u5b89\\u5b9a\\u3055\\u305b\\u308b\\u5834\\u9762\\u304c\\u3042\\u308a\\u307e\\u3059\\u3002\\u4f8b\\u3048\\u3070\\u3001\\u30b5\\u30e1\\u306f\\u6c34\\u4e2d\\u3067\\u5b89\\u5b9a\\u3057\\u305f\\u4f4d\\u7f6e\\u306b\\u3068\\u3069\\u307e\\u308b\\u3053\\u3068\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    },\n",
      "    {\n",
      "      \"index\": 1,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"\\u9b5a\\u306f\\u7761\\u7720\\u3068\\u3044\\u3046\\u3082\\u306e\\u3092\\u884c\\u3046\\u751f\\u7269\\u3067\\u306f\\u3042\\u308a\\u307e\\u305b\\u3093\\u3002\\u4ee3\\u308f\\u308a\\u306b\\u3001\\u9b5a\\u306f\\u4f11\\u606f\\u3092\\u3068\\u308b\\u305f\\u3081\\u306b\\u6c34\\u4e2d\\u3067\\u9759\\u6b62\\u3057\\u305f\\u308a\\u3001\\u307b\\u304b\\u306e\\u9b5a\\u3068\\u4e00\\u7dd2\\u306b\\u7fa4\\u308c\\u3092\\u4f5c\\u3063\\u305f\\u308a\\u3057\\u307e\\u3059\\u3002\\u3053\\u308c\\u3089\\u306e\\u884c\\u52d5\\u306f\\u3001\\u9b5a\\u304c\\u30a8\\u30cd\\u30eb\\u30ae\\u30fc\\u3092\\u7bc0\\u7d04\\u3057\\u3001\\u8eab\\u4f53\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 17,\n",
      "    \"completion_tokens\": 200,\n",
      "    \"total_tokens\": 217\n",
      "  },\n",
      "  \"system_fingerprint\": \"fp_3b956da36b\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "responce = openai.ChatCompletion.create(\n",
    "    model =\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\":\"魚はいつ眠る？\"\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=1,\n",
    "    n=2,\n",
    ")\n",
    "\n",
    "print(responce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbc965f5-25ee-464e-9ad7-6d79a71faf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.schema import AIMessage\n",
    "from langchain.schema import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d52895-c3c2-4ed6-9030-5f5a0f5e6d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hartm\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\hartm\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは！元気ですか？何かお手伝いできることがありますか？\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "#クライアント作成、呼び出すモデル指定\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "#実行\n",
    "result = chat(\n",
    "    [\n",
    "        HumanMessage(content=\"こんにちは！\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4371dd88-7852-4507-8238-76cdd8e8528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat(\n",
    "    [\n",
    "        HumanMessage(content=\"だし巻き卵の作り方教えて\"),\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b3da4c0-7990-43e4-8f1c-496b03f52591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代表的な製品=>iPhone\n",
      "代表的な製品=>iPad\n",
      "代表的な製品=>MacBook\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "#実行\n",
    "result = chat(\n",
    "    [\n",
    "        HumanMessage(content=\"Appleが開発した代表的な製品を３つ教えてください\"),\n",
    "        HumanMessage(content=output_parser.get_format_instructions()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "output = output_parser.parse(result.content)\n",
    "\n",
    "for item in output:\n",
    "    print(\"代表的な製品=>\" + item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "484c7c57-0546-40ef-816c-4c16af56769a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hartm\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\hartm\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "食べれました\n",
      "\n",
      "今日のラーメンはとてもおいしく、満足のいくものでした\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "#モデルを指定\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "result = llm(\n",
    "    #入力テキスト\n",
    "    \"美味しいラーメンを\",\n",
    "    #生成をストップさせる文字列\n",
    "    stop = \"。\"\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e5537-6cf2-4e01-89ac-f28c8fcde3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ローカルで使用可能なモデル\n",
    "from langchain.llms import GPT4All\n",
    "\n",
    "llm = GPT4All()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee815b16-9539-49bc-919a-46f3c5eee347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは！元気ですか？何かお手伝いできることがあればお知らせくださいね。\n",
      "実行時間：1.1649365425109863秒\n",
      "こんにちは！元気ですか？何かお手伝いできることがあればお知らせくださいね。\n",
      "実行時間：0.0秒\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "start = time.time()\n",
    "result = chat([\n",
    "    HumanMessage(content=\"こんにちは！\")\n",
    "])\n",
    "end = time.time()\n",
    "\n",
    "print(result.content)\n",
    "print(f\"実行時間：{end -start}秒\")\n",
    "\n",
    "start = time.time()\n",
    "result = chat([\n",
    "    HumanMessage(content=\"こんにちは！\")\n",
    "])\n",
    "end = time.time()\n",
    "\n",
    "print(result.content)\n",
    "print(f\"実行時間：{end -start}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7883ba59-7dff-466e-951d-0cc79e12b093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "美味しいステーキを焼くための基本的な方法を以下に示します。\n",
      "\n",
      "1. ステーキを室温に戻す：冷蔵庫から出したステーキを室温に約30分～1時間程度置いておきます。これにより、ステーキの中心部まで均一に温まります。\n",
      "\n",
      "2. ステーキに塩と胡椒を振る：ステーキの両面に適量の塩と胡椒を振ります。ここで、香辛料やハーブなどを加えるのもおすすめです。\n",
      "\n",
      "3. フライパンを熱する：中火～中高火力でフライパンをしっかりと予熱します。ステーキが焼けるようになるまで、約5分程度かかります。\n",
      "\n",
      "4. ステーキを焼く：フライパンにオリーブオイルやバターを加え、ステーキを両面焼きます。焼き加減はお好みで、レア、ミディアム、ウェルダンなど好みの焼き加減に調整します。\n",
      "\n",
      "5. 休ませる：ステーキを焼き上がったら、アルミホイルで覆い、約5分程度休ませます。これにより、肉汁が均等に行き渡り、ジューシーな味わいになります。\n",
      "\n",
      "以上が、美味しいステーキの基本的な焼き方です。焼き加減や調理法は個々の好みによって異なりますので、自分好みのステーキを焼くためには試行錯誤してみてください。美味しいステーキを焼くための基本的な方法を以下に示します。\n",
      "\n",
      "1. ステーキを室温に戻す：冷蔵庫から出したステーキを室温に約30分～1時間程度置いておきます。これにより、ステーキの中心部まで均一に温まります。\n",
      "\n",
      "2. ステーキに塩と胡椒を振る：ステーキの両面に適量の塩と胡椒を振ります。ここで、香辛料やハーブなどを加えるのもおすすめです。\n",
      "\n",
      "3. フライパンを熱する：中火～中高火力でフライパンをしっかりと予熱します。ステーキが焼けるようになるまで、約5分程度かかります。\n",
      "\n",
      "4. ステーキを焼く：フライパンにオリーブオイルやバターを加え、ステーキを両面焼きます。焼き加減はお好みで、レア、ミディアム、ウェルダンなど好みの焼き加減に調整します。\n",
      "\n",
      "5. 休ませる：ステーキを焼き上がったら、アルミホイルで覆い、約5分程度休ませます。これにより、肉汁が均等に行き渡り、ジューシーな味わいになります。\n",
      "\n",
      "以上が、美味しいステーキの基本的な焼き方です。焼き加減や調理法は個々の好みによって異なりますので、自分好みのステーキを焼くためには試行錯誤してみてください。\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "resp = chat([\n",
    "    HumanMessage(content=\"美味しいステーキの焼き方教えて\")\n",
    "])\n",
    "\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c85288-5d42-464b-a79b-7ea62a73276d",
   "metadata": {},
   "source": [
    "Geminiの場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a52c2e11-b125-4ac7-b263-9a815a4a311d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは！ご機嫌はいかがですか？\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "\n",
    "GOOGLE_API_KEY = os.environ['GOOGLE_API_KEY']\n",
    "\n",
    "#クライアント作成、呼び出すモデル指定\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-pro\"\n",
    ")\n",
    "#実行\n",
    "result = llm(\n",
    "    [\n",
    "        HumanMessage(content=\"こんにちは！\")\n",
    "    ]\n",
    ")\n",
    "#出力\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118605af-e5aa-4610-b45e-c046723caaae",
   "metadata": {},
   "source": [
    "OpenAI、Gemini共通"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f2d0198-44aa-42ff-bae0-dafd5748b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ユーザー、アシスタント、システムメッセージをインポート\n",
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    AIMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94355914-6410-4792-8932-75b549c2cd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Ingredients:**\n",
      "\n",
      "- 2 large eggs\n",
      "- 1/4 cup dashi (Japanese soup stock)\n",
      "- 1 tablespoon soy sauce\n",
      "- 1 tablespoon mirin (Japanese sweet cooking wine)\n",
      "- 1/4 teaspoon sugar\n",
      "- Vegetable oil, for greasing the pan\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. In a medium bowl, whisk together the eggs, dashi, soy sauce, mirin, and sugar.\n",
      "2. Heat a nonstick skillet over medium heat. Grease the pan with vegetable oil.\n",
      "3. Pour a thin layer of the egg mixture into the heated pan.\n",
      "4. Cook for 1-2 minutes, or until the edges are set and the center is slightly runny.\n",
      "5. Using a spatula, gently roll up the omelet from one end to the other.\n",
      "6. Slide the rolled omelet to one side of the pan.\n",
      "7. Pour another thin layer of egg mixture into the pan and cook as before.\n",
      "8. Repeat steps 5-7 until all of the egg mixture is used up.\n",
      "9. Transfer the omelet to a cutting board and let it cool slightly before slicing and serving.\n"
     ]
    }
   ],
   "source": [
    "result = llm(\n",
    "    [\n",
    "        HumanMessage(content=\"だし巻き卵の作り方教えて\"),\n",
    "        AIMessage(content=\"{llmからの返答であるだし巻き卵の作り方}\"),\n",
    "        HumanMessage(content=\"英語に翻訳して\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17419064-43d2-4093-88aa-09f8745526f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**朝食**\n",
      "\n",
      "* オートミール、ベリー、ナッツ入り\n",
      "* 全粒粉トースト、アボカド、卵\n",
      "* ギリシャヨーグルト、フルーツ、グラノーラ\n",
      "\n",
      "**昼食**\n",
      "\n",
      "* サラダ（葉物野菜、グリルした鶏肉、全粒穀物、野菜）\n",
      "* サンドイッチ（全粒粉パン、赤身のタンパク質、野菜）\n",
      "* スープとサラダの組み合わせ\n",
      "\n",
      "**夕食**\n",
      "\n",
      "* 焼き鮭、玄米、蒸し野菜\n",
      "* 鶏肉のグリル、焼き芋、ブロッコリー\n",
      "* レンズ豆のスープ、全粒粉パン\n",
      "\n",
      "**間食**\n",
      "\n",
      "* フルーツ（りんご、バナナ、ベリー）\n",
      "* 野菜（ニンジン、セロリ、キュウリ）\n",
      "* ヨーグルト\n",
      "* ナッツと種\n",
      "\n",
      "**追加のヒント**\n",
      "\n",
      "* 1日に5～6回の少量の食事を食べる。\n",
      "* 全粒穀物、果物、野菜を多く含める。\n",
      "* 赤身のタンパク質（鶏肉、魚、豆類）を選ぶ。\n",
      "* 砂糖分の多い飲み物や加工食品は避ける。\n",
      "* 食物繊維を十分に摂取する（1日あたり25～30グラム）。\n",
      "* 食事療法を管理するために血糖値を定期的に測定する。\n",
      "\n",
      "**留意事項**\n",
      "\n",
      "この献立はあくまでサンプルであり、個々のニーズによって調整する必要があります。糖尿病患者は、食事療法を計画する前に医療従事者に相談することが重要です。\n"
     ]
    }
   ],
   "source": [
    "result = llm(\n",
    "    [\n",
    "        SystemMessage(content=\"あなたは管理栄養士です\"),\n",
    "        HumanMessage(content=\"糖尿病患者むけの献立を考えてください\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bed0eba9-11f0-4a75-a5aa-b0195d6ff5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14a2160c-85b4-4fbd-a22a-4443c369d7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhoneはどこの会社が開発した製品ですか？\n",
      "AQUOSはどこの会社が開発した製品ですか？\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"{product}はどこの会社が開発した製品ですか？\",\n",
    "    input_variables=[\n",
    "        \"product\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(prompt.format(product=\"iPhone\"))\n",
    "print(prompt.format(product=\"AQUOS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f03d9a1-c7fc-4426-af76-a158871071b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#エラーになる\n",
    "print(prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e315197e-f240-4d22-8bf3-b1233338eae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "#クライアント作成、呼び出すモデル指定\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-pro\"\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{product}はどこの会社が開発した製品ですか？\",\n",
    "    input_variables=[\n",
    "        \"product\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = llm(\n",
    "    [\n",
    "        HumanMessage(content=prompt.format(product=\"iPhone\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "800d47bb-cf1b-4b2f-9b81-10118fe05322",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_json = prompt.save(\"prompt.json\")\n",
    "prompt_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd407dc-8f10-44c0-b00b-e7fca96e9668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AQUOSはどこの会社が開発した製品ですか？\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "\n",
    "loaded_prompt = load_prompt(\"prompt.json\")\n",
    "\n",
    "print(loaded_prompt.format(product=\"AQUOS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ef1d292-5b14-4494-8fe1-432195429245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代表的な製品=>1. iPhone\n",
      "2. iPad\n",
      "3. Mac\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "#実行\n",
    "result = llm(\n",
    "    [\n",
    "        HumanMessage(content=\"Appleが開発した代表的な製品を３つ教えてください\"),\n",
    "        #Geminiでは、このパラメータでエラーになる\n",
    "        #HumanMessage(content=output_parser.get_format_instructions()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "output = output_parser.parse(result.content)\n",
    "\n",
    "for item in output:\n",
    "    print(\"代表的な製品=>\" + item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f9c1f63-430e-47e4-bbc8-266f17333961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは！ご質問は何ですか？お手伝いいたします。\n",
      "実行時間：1.2244462966918945秒\n",
      "もう一度こんにちは！ご質問は何でしょうか？お手伝いさせていただきます。\n",
      "実行時間：1.1247618198394775秒\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "#chat = ChatOpenAI()\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "chat = model.start_chat()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\"\"\"\n",
    "result = chat([\n",
    "    HumanMessage(content=\"こんにちは！\")\n",
    "])\n",
    "\"\"\"\n",
    "\n",
    "result = chat.send_message(\"こんにちは！\")\n",
    "end = time.time()\n",
    "\n",
    "print(result.text)\n",
    "print(f\"実行時間：{end -start}秒\")\n",
    "\n",
    "start = time.time()\n",
    "\"\"\"\n",
    "result = chat([\n",
    "    HumanMessage(content=\"こんにちは！\")\n",
    "])\n",
    "\"\"\"\n",
    "result = chat.send_message(\"こんにちは！\")\n",
    "end = time.time()\n",
    "\n",
    "print(result.text)\n",
    "print(f\"実行時間：{end -start}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9d08454-e248-474b-bd25-46819bdfc591",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_google_genai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[1;32m----> 7\u001b[0m chat \u001b[38;5;241m=\u001b[39m \u001b[43mChatGoogleGenerativeAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mStreamingStdOutCallbackHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\load\\serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\main.py:1102\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:502\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k must be positive\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 502\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    503\u001b[0m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(model_name\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[1;31mKeyError\u001b[0m: 'model'"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "\n",
    "\n",
    "chat = ChatGoogleGenerativeAI(\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "resp = chat([\n",
    "    HumanMessage(content=\"美味しいステーキの焼き方教えて\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec87b88-f5c9-4ac5-8dbe-f2b9029c8479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
