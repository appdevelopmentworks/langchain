{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95075c49-04fa-49eb-b83a-777f4ed52910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-9L6q6x7HCDtqe8gjBEvhHh8wAiDQQ\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1714818206,\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"\\u9b5a\\u306f\\u57fa\\u672c\\u7684\\u306b\\u306f\\u6c34\\u4e2d\\u3067\\u305a\\u3063\\u3068\\u6cf3\\u304e\\u7d9a\\u3051\\u3066\\u3044\\u308b\\u305f\\u3081\\u3001\\u7720\\u308b\\u3068\\u3044\\u3046\\u6982\\u5ff5\\u304c\\u3042\\u307e\\u308a\\u3042\\u308a\\u307e\\u305b\\u3093\\u3002\\u305f\\u3060\\u3057\\u3001\\u4e00\\u90e8\\u306e\\u9b5a\\u985e\\u306f\\u4f11\\u606f\\u306e\\u305f\\u3081\\u306b\\u4f53\\u3092\\u5b89\\u5b9a\\u3055\\u305b\\u308b\\u5834\\u9762\\u304c\\u3042\\u308a\\u307e\\u3059\\u3002\\u4f8b\\u3048\\u3070\\u3001\\u30b5\\u30e1\\u306f\\u6c34\\u4e2d\\u3067\\u5b89\\u5b9a\\u3057\\u305f\\u4f4d\\u7f6e\\u306b\\u3068\\u3069\\u307e\\u308b\\u3053\\u3068\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    },\n",
      "    {\n",
      "      \"index\": 1,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"\\u9b5a\\u306f\\u7761\\u7720\\u3068\\u3044\\u3046\\u3082\\u306e\\u3092\\u884c\\u3046\\u751f\\u7269\\u3067\\u306f\\u3042\\u308a\\u307e\\u305b\\u3093\\u3002\\u4ee3\\u308f\\u308a\\u306b\\u3001\\u9b5a\\u306f\\u4f11\\u606f\\u3092\\u3068\\u308b\\u305f\\u3081\\u306b\\u6c34\\u4e2d\\u3067\\u9759\\u6b62\\u3057\\u305f\\u308a\\u3001\\u307b\\u304b\\u306e\\u9b5a\\u3068\\u4e00\\u7dd2\\u306b\\u7fa4\\u308c\\u3092\\u4f5c\\u3063\\u305f\\u308a\\u3057\\u307e\\u3059\\u3002\\u3053\\u308c\\u3089\\u306e\\u884c\\u52d5\\u306f\\u3001\\u9b5a\\u304c\\u30a8\\u30cd\\u30eb\\u30ae\\u30fc\\u3092\\u7bc0\\u7d04\\u3057\\u3001\\u8eab\\u4f53\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 17,\n",
      "    \"completion_tokens\": 200,\n",
      "    \"total_tokens\": 217\n",
      "  },\n",
      "  \"system_fingerprint\": \"fp_3b956da36b\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "responce = openai.ChatCompletion.create(\n",
    "    model =\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\":\"魚はいつ眠る？\"\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=1,\n",
    "    n=2,\n",
    ")\n",
    "\n",
    "print(responce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbc965f5-25ee-464e-9ad7-6d79a71faf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.schema import AIMessage\n",
    "from langchain.schema import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d52895-c3c2-4ed6-9030-5f5a0f5e6d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hartm\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\hartm\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは！元気ですか？何かお手伝いできることがあれば教えてくださいね。\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "#クライアント作成、呼び出すモデル指定\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "#実行\n",
    "result = chat(\n",
    "    [\n",
    "        HumanMessage(content=\"こんにちは！\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4371dd88-7852-4507-8238-76cdd8e8528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat(\n",
    "    [\n",
    "        HumanMessage(content=\"だし巻き卵の作り方教えて\"),\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b3da4c0-7990-43e4-8f1c-496b03f52591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代表的な製品=>iPhone\n",
      "代表的な製品=>iPad\n",
      "代表的な製品=>MacBook\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "#実行\n",
    "result = chat(\n",
    "    [\n",
    "        HumanMessage(content=\"Appleが開発した代表的な製品を３つ教えてください\"),\n",
    "        HumanMessage(content=output_parser.get_format_instructions()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "output = output_parser.parse(result.content)\n",
    "\n",
    "for item in output:\n",
    "    print(\"代表的な製品=>\" + item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "484c7c57-0546-40ef-816c-4c16af56769a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hartm\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\hartm\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "お届けすることができるよう、日々努力しています！\n",
      "\n",
      "当店では、自家製麺をはじめとしたこだわりの食材を使用し、手間暇かけて丁寧に作ったラーメンを提供しています\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "#モデルを指定\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "result = llm(\n",
    "    #入力テキスト\n",
    "    \"美味しいラーメンを\",\n",
    "    #生成をストップさせる文字列\n",
    "    stop = \"。\"\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e5537-6cf2-4e01-89ac-f28c8fcde3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ローカルで使用可能なモデル\n",
    "from langchain.llms import GPT4All\n",
    "\n",
    "llm = GPT4All()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee815b16-9539-49bc-919a-46f3c5eee347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは！元気ですか？何かお手伝いできることがありますか？\n",
      "実行時間：0.9285881519317627秒\n",
      "こんにちは！元気ですか？何かお手伝いできることがありますか？\n",
      "実行時間：0.0秒\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "start = time.time()\n",
    "result = chat([\n",
    "    HumanMessage(content=\"こんにちは！\")\n",
    "])\n",
    "end = time.time()\n",
    "\n",
    "print(result.content)\n",
    "print(f\"実行時間：{end -start}秒\")\n",
    "\n",
    "start = time.time()\n",
    "result = chat([\n",
    "    HumanMessage(content=\"こんにちは！\")\n",
    "])\n",
    "end = time.time()\n",
    "\n",
    "print(result.content)\n",
    "print(f\"実行時間：{end -start}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7883ba59-7dff-466e-951d-0cc79e12b093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "美味しいステーキを焼くためには、以下の手順を参考にしてください。\n",
      "\n",
      "1. ステーキを室温に戻す：ステーキを冷蔵庫から出して、室温になるまで放置します。これにより、ステーキが均一に焼けます。\n",
      "\n",
      "2. ステーキを塩コショウで下味をつける：ステーキの両面に塩とコショウをふりかけ、軽く押し込んで下味をつけます。塩は焼く前につけることでジューシーさを引き出し、コショウは香りを引き立てます。\n",
      "\n",
      "3. フライパンを熱する：フライパンを中火から高火にかけて熱し、ステーキを焼く準備をします。\n",
      "\n",
      "4. ステーキを焼く：ステーキをフライパンに入れて、片面を2〜3分ずつ焼きます。焼き色がついたら裏返し、同じく2〜3分焼きます。焼き加減はお好みで調整してください。\n",
      "\n",
      "5. 休ませる：ステーキを焼き終わったら、アルミホイルで包んで5分程度休ませます。これにより、肉汁がしっかりと広がり、ジューシーで柔らかい仕上がりになります。\n",
      "\n",
      "6. 切って食べる：ステーキナイフで適当な厚さにステーキを切り分け、お好みのソースや塩を添えてお召し上がりください。\n",
      "\n",
      "以上が美味しいステーキの焼き方です。ぜひお試しください。美味しいステーキを焼くためには、以下の手順を参考にしてください。\n",
      "\n",
      "1. ステーキを室温に戻す：ステーキを冷蔵庫から出して、室温になるまで放置します。これにより、ステーキが均一に焼けます。\n",
      "\n",
      "2. ステーキを塩コショウで下味をつける：ステーキの両面に塩とコショウをふりかけ、軽く押し込んで下味をつけます。塩は焼く前につけることでジューシーさを引き出し、コショウは香りを引き立てます。\n",
      "\n",
      "3. フライパンを熱する：フライパンを中火から高火にかけて熱し、ステーキを焼く準備をします。\n",
      "\n",
      "4. ステーキを焼く：ステーキをフライパンに入れて、片面を2〜3分ずつ焼きます。焼き色がついたら裏返し、同じく2〜3分焼きます。焼き加減はお好みで調整してください。\n",
      "\n",
      "5. 休ませる：ステーキを焼き終わったら、アルミホイルで包んで5分程度休ませます。これにより、肉汁がしっかりと広がり、ジューシーで柔らかい仕上がりになります。\n",
      "\n",
      "6. 切って食べる：ステーキナイフで適当な厚さにステーキを切り分け、お好みのソースや塩を添えてお召し上がりください。\n",
      "\n",
      "以上が美味しいステーキの焼き方です。ぜひお試しください。\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "resp = chat([\n",
    "    HumanMessage(content=\"美味しいステーキの焼き方教えて\")\n",
    "])\n",
    "\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4ab3604-9f53-4da9-97b8-cf447ab6199f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hartm\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formatted_prompt:  以下の句読点の抜けた入力に句読点を追加してください。追加して良い句読点は「、」「。」のみです。他の句読点は追加しないでください。\n",
      "\n",
      "入力: LangChainはChatGPT・Large Language Model (LLM)の実利用をより柔軟に簡易に行うためのツール群です\n",
      "出力: LangChainは、ChatGPT・Large Language Model (LLM)の実利用をより柔軟に、簡易に行うためのツール群です。\n",
      "\n",
      "入力: 私はさまざまな機能がモジュールとして提供されているLangChainを使ってアプリケーションを開発しています\n",
      "出力:\n",
      "result:   私は、さまざまな機能がモジュールとして提供されているLangChainを使って、アプリケーションを開発しています。\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"LangChainはChatGPT・Large Language Model (LLM)の実利用をより柔軟に簡易に行うためのツール群です\",  #← 入力例\n",
    "        \"output\": \"LangChainは、ChatGPT・Large Language Model (LLM)の実利用をより柔軟に、簡易に行うためのツール群です。\"  #← 出力例\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = PromptTemplate(  #← PromptTemplateの準備\n",
    "    input_variables=[\"input\", \"output\"],  #← inputとoutputを入力変数として設定\n",
    "    template=\"入力: {input}\\n出力: {output}\",  #← テンプレート\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(  #← FewShotPromptTemplateの準備\n",
    "    examples=examples,  #← 入力例と出力例を定義\n",
    "    example_prompt=prompt,  #← FewShotPromptTemplateにPromptTemplateを渡す\n",
    "    prefix=\"以下の句読点の抜けた入力に句読点を追加してください。追加して良い句読点は「、」「。」のみです。他の句読点は追加しないでください。\",  #← 指示を追加する\n",
    "    suffix=\"入力: {input_string}\\n出力:\",  #← 出力例の入力変数を定義\n",
    "    input_variables=[\"input_string\"],  #← FewShotPromptTemplateの入力変数を設定\n",
    ")\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-3.5-turbo-instruct\"\n",
    ")\n",
    "formatted_prompt = few_shot_prompt.format( #← FewShotPromptTemplateを使ってプロンプトを作成\n",
    "    input_string=\"私はさまざまな機能がモジュールとして提供されているLangChainを使ってアプリケーションを開発しています\"\n",
    ")\n",
    "result = llm.predict(formatted_prompt)\n",
    "print(\"formatted_prompt: \", formatted_prompt)\n",
    "print(\"result: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a784a87-ae28-4a20-b8c3-98ce72863dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1518-10-08 14:29:50.568436\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import DatetimeOutputParser  #← Output ParserであるDatetimeOutputParserをインポート\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "output_parser = DatetimeOutputParser() #← DatetimeOutputParserを初期化\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", )\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"{product}のリリース日を教えて\") #← リリース日を聞く\n",
    "\n",
    "result = chat(\n",
    "    [\n",
    "        HumanMessage(content=prompt.format(product=\"iPhone8\")),  #← iPhone8のリリース日を聞く\n",
    "        HumanMessage(content=output_parser.get_format_instructions()),  #← output_parser.get_format_instructions()を実行し、言語モデルへの指示を追加する\n",
    "    ]\n",
    ")\n",
    "\n",
    "output = output_parser.parse(result.content) #← 出力結果を解析して日時形式に変換する\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490605dc-1fb6-45a8-91e4-b8d93fe4452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "#GeminiのLangChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "output_parser = DatetimeOutputParser()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"{product}のリリース日を教えて\") #← リリース日を聞く\n",
    "\n",
    "result = chat(\n",
    "    [\n",
    "        HumanMessage(content=prompt.format(product=\"iPhone8\")),\n",
    "        HumanMessage(content=output_parser.get_format_instructions()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "output = output_parser.parse(result.content)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e742947-0c16-4e6a-88d2-c4b91105f535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデル名: Samsung Galaxy S21 Ultra\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.schema import HumanMessage\n",
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "class Smartphone(BaseModel): #← Pydanticのモデルを定義する\n",
    "    release_date: str = Field(description=\"スマートフォンの発売日\") #← Fieldを使って説明を追加する\n",
    "    screen_inches: float = Field(description=\"スマートフォンの画面サイズ(インチ)\")\n",
    "    os_installed: str = Field(description=\"スマートフォンにインストールされているOS\")\n",
    "    model_name: str = Field(description=\"スマートフォンのモデル名\")\n",
    "\n",
    "    @validator(\"screen_inches\") #← validatorを使って値を検証する\n",
    "    def validate_screen_inches(cls, field): #← validatorの引数には、検証するフィールドと値が渡される\n",
    "        if field <= 0: #← screen_inchesが0以下の場合はエラーを返す\n",
    "            raise ValueError(\"Screen inches must be a positive number\")\n",
    "        return field\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Smartphone) #← PydanticOutputParserをSmartPhoneモデルで初期化する\n",
    "\n",
    "result = chat([ #← Chat modelsにHumanMessageを渡して、文章を生成する\n",
    "    HumanMessage(content=\"Androidでリリースしたスマートフォンを1個挙げて\"),\n",
    "    HumanMessage(content=parser.get_format_instructions())\n",
    "])\n",
    "\n",
    "parsed_result = parser.parse(result.content) #← PydanticOutputParserを使って、文章をパースする\n",
    "\n",
    "print(f\"モデル名: {parsed_result.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00dc5781-c997-4f33-b878-574d7fb0aa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデル名: Samsung Galaxy S21 Ultra\n",
      "画面サイズ: 6.7インチ\n",
      "OS: Android 11\n",
      "スマートフォンの発売日: 2021-05-15\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import OutputFixingParser  #←OutputFixingParserを追加\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.schema import HumanMessage\n",
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "class Smartphone(BaseModel):\n",
    "    release_date: str = Field(description=\"スマートフォンの発売日\")\n",
    "    screen_inches: float = Field(description=\"スマートフォンの画面サイズ(インチ)\")\n",
    "    os_installed: str = Field(description=\"スマートフォンにインストールされているOS\")\n",
    "    model_name: str = Field(description=\"スマートフォンのモデル名\")\n",
    "\n",
    "    @validator(\"screen_inches\")\n",
    "    def validate_screen_inches(cls, field):\n",
    "        if field <= 0:\n",
    "            raise ValueError(\"Screen inches must be a positive number\")\n",
    "        return field\n",
    "\n",
    "\n",
    "parser = OutputFixingParser.from_llm(  #← OutputFixingParserを使用するように書き換え\n",
    "    parser=PydanticOutputParser(pydantic_object=Smartphone),  #← parserを設定\n",
    "    llm=chat  #← 修正に使用する言語モデルを設定\n",
    ")\n",
    "\n",
    "result = chat([HumanMessage(content=\"Androidでリリースしたスマートフォンを1個挙げて\"), HumanMessage(content=parser.get_format_instructions())])\n",
    "\n",
    "parsed_result = parser.parse(result.content)\n",
    "\n",
    "print(f\"モデル名: {parsed_result.model_name}\")\n",
    "print(f\"画面サイズ: {parsed_result.screen_inches}インチ\")\n",
    "print(f\"OS: {parsed_result.os_installed}\")\n",
    "print(f\"スマートフォンの発売日: {parsed_result.release_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c85288-5d42-464b-a79b-7ea62a73276d",
   "metadata": {},
   "source": [
    "# Geminiの場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a52c2e11-b125-4ac7-b263-9a815a4a311d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは！ご機嫌はいかがですか？\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "\n",
    "GOOGLE_API_KEY = os.environ['GOOGLE_API_KEY']\n",
    "\n",
    "#クライアント作成、呼び出すモデル指定\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-pro\"\n",
    ")\n",
    "#実行\n",
    "result = llm(\n",
    "    [\n",
    "        HumanMessage(content=\"こんにちは！\")\n",
    "    ]\n",
    ")\n",
    "#出力\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118605af-e5aa-4610-b45e-c046723caaae",
   "metadata": {},
   "source": [
    "OpenAI、Gemini共通"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f2d0198-44aa-42ff-bae0-dafd5748b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ユーザー、アシスタント、システムメッセージをインポート\n",
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    AIMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94355914-6410-4792-8932-75b549c2cd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Ingredients:**\n",
      "\n",
      "- 2 large eggs\n",
      "- 1/4 cup dashi (Japanese soup stock)\n",
      "- 1 tablespoon soy sauce\n",
      "- 1 tablespoon mirin (Japanese sweet cooking wine)\n",
      "- 1/4 teaspoon sugar\n",
      "- Vegetable oil, for greasing the pan\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. In a medium bowl, whisk together the eggs, dashi, soy sauce, mirin, and sugar.\n",
      "2. Heat a nonstick skillet over medium heat. Grease the pan with vegetable oil.\n",
      "3. Pour a thin layer of the egg mixture into the heated pan.\n",
      "4. Cook for 1-2 minutes, or until the edges are set and the center is slightly runny.\n",
      "5. Using a spatula, gently roll up the omelet from one end to the other.\n",
      "6. Slide the rolled omelet to one side of the pan.\n",
      "7. Pour another thin layer of egg mixture into the pan and cook as before.\n",
      "8. Repeat steps 5-7 until all of the egg mixture is used up.\n",
      "9. Transfer the omelet to a cutting board and let it cool slightly before slicing and serving.\n"
     ]
    }
   ],
   "source": [
    "result = llm(\n",
    "    [\n",
    "        HumanMessage(content=\"だし巻き卵の作り方教えて\"),\n",
    "        AIMessage(content=\"{llmからの返答であるだし巻き卵の作り方}\"),\n",
    "        HumanMessage(content=\"英語に翻訳して\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17419064-43d2-4093-88aa-09f8745526f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**朝食**\n",
      "\n",
      "* オートミール、ベリー、ナッツ入り\n",
      "* 全粒粉トースト、アボカド、卵\n",
      "* ギリシャヨーグルト、フルーツ、グラノーラ\n",
      "\n",
      "**昼食**\n",
      "\n",
      "* サラダ（葉物野菜、グリルした鶏肉、全粒穀物、野菜）\n",
      "* サンドイッチ（全粒粉パン、赤身のタンパク質、野菜）\n",
      "* スープとサラダの組み合わせ\n",
      "\n",
      "**夕食**\n",
      "\n",
      "* 焼き鮭、玄米、蒸し野菜\n",
      "* 鶏肉のグリル、焼き芋、ブロッコリー\n",
      "* レンズ豆のスープ、全粒粉パン\n",
      "\n",
      "**間食**\n",
      "\n",
      "* フルーツ（りんご、バナナ、ベリー）\n",
      "* 野菜（ニンジン、セロリ、キュウリ）\n",
      "* ヨーグルト\n",
      "* ナッツと種\n",
      "\n",
      "**追加のヒント**\n",
      "\n",
      "* 1日に5～6回の少量の食事を食べる。\n",
      "* 全粒穀物、果物、野菜を多く含める。\n",
      "* 赤身のタンパク質（鶏肉、魚、豆類）を選ぶ。\n",
      "* 砂糖分の多い飲み物や加工食品は避ける。\n",
      "* 食物繊維を十分に摂取する（1日あたり25～30グラム）。\n",
      "* 食事療法を管理するために血糖値を定期的に測定する。\n",
      "\n",
      "**留意事項**\n",
      "\n",
      "この献立はあくまでサンプルであり、個々のニーズによって調整する必要があります。糖尿病患者は、食事療法を計画する前に医療従事者に相談することが重要です。\n"
     ]
    }
   ],
   "source": [
    "result = llm(\n",
    "    [\n",
    "        SystemMessage(content=\"あなたは管理栄養士です\"),\n",
    "        HumanMessage(content=\"糖尿病患者むけの献立を考えてください\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bed0eba9-11f0-4a75-a5aa-b0195d6ff5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14a2160c-85b4-4fbd-a22a-4443c369d7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhoneはどこの会社が開発した製品ですか？\n",
      "AQUOSはどこの会社が開発した製品ですか？\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"{product}はどこの会社が開発した製品ですか？\",\n",
    "    input_variables=[\n",
    "        \"product\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(prompt.format(product=\"iPhone\"))\n",
    "print(prompt.format(product=\"AQUOS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f03d9a1-c7fc-4426-af76-a158871071b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#エラーになる\n",
    "print(prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e315197e-f240-4d22-8bf3-b1233338eae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "#クライアント作成、呼び出すモデル指定\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-pro\"\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{product}はどこの会社が開発した製品ですか？\",\n",
    "    input_variables=[\n",
    "        \"product\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = llm(\n",
    "    [\n",
    "        HumanMessage(content=prompt.format(product=\"iPhone\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "800d47bb-cf1b-4b2f-9b81-10118fe05322",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_json = prompt.save(\"prompt.json\")\n",
    "prompt_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd407dc-8f10-44c0-b00b-e7fca96e9668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AQUOSはどこの会社が開発した製品ですか？\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "\n",
    "loaded_prompt = load_prompt(\"prompt.json\")\n",
    "\n",
    "print(loaded_prompt.format(product=\"AQUOS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ef1d292-5b14-4494-8fe1-432195429245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代表的な製品=>1. iPhone\n",
      "2. iPad\n",
      "3. Mac\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "#実行\n",
    "result = llm(\n",
    "    [\n",
    "        HumanMessage(content=\"Appleが開発した代表的な製品を３つ教えてください\"),\n",
    "        #Geminiでは、このパラメータでエラーになる\n",
    "        #HumanMessage(content=output_parser.get_format_instructions()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "output = output_parser.parse(result.content)\n",
    "\n",
    "for item in output:\n",
    "    print(\"代表的な製品=>\" + item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f9c1f63-430e-47e4-bbc8-266f17333961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは！ご質問は何ですか？お手伝いいたします。\n",
      "実行時間：1.2244462966918945秒\n",
      "もう一度こんにちは！ご質問は何でしょうか？お手伝いさせていただきます。\n",
      "実行時間：1.1247618198394775秒\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "#chat = ChatOpenAI()\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "chat = model.start_chat()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\"\"\"\n",
    "result = chat([\n",
    "    HumanMessage(content=\"こんにちは！\")\n",
    "])\n",
    "\"\"\"\n",
    "\n",
    "result = chat.send_message(\"こんにちは！\")\n",
    "end = time.time()\n",
    "\n",
    "print(result.text)\n",
    "print(f\"実行時間：{end -start}秒\")\n",
    "\n",
    "start = time.time()\n",
    "\"\"\"\n",
    "result = chat([\n",
    "    HumanMessage(content=\"こんにちは！\")\n",
    "])\n",
    "\"\"\"\n",
    "result = chat.send_message(\"こんにちは！\")\n",
    "end = time.time()\n",
    "\n",
    "print(result.text)\n",
    "print(f\"実行時間：{end -start}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9d08454-e248-474b-bd25-46819bdfc591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hartm\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "\n",
    "\n",
    "chat = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-pro\",\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "resp = chat([\n",
    "    HumanMessage(content=\"美味しいステーキの焼き方教えて\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec87b88-f5c9-4ac5-8dbe-f2b9029c8479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
