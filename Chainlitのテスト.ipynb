{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "058afcc8-ab28-488d-af4f-85ecfeeda1e2",
   "metadata": {},
   "source": [
    "## Chainlitの練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dcf564-c080-4469-9f1b-c176e86b693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#オウム返し\n",
    "import chainlit as cl\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message: cl.Message):\n",
    "    # Your custom logic goes here...\n",
    "\n",
    "    # Send a response back to the user\n",
    "    await cl.Message(\n",
    "        content=f\"Received: {message.content}\",\n",
    "    ).send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3716da43-67fe-4cb9-aea7-a637d7200a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainlit as cl\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "#モデルを設定\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "#バッファーを初期化\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message: cl.Message):\n",
    "    # Your custom logic goes here...\n",
    "    chat = ChatOpenAI(\n",
    "        #ストリーミングモード指定\n",
    "        streaming=True,\n",
    "        callbacks=[\n",
    "            StreamingStdOutCallbackHandler()\n",
    "        ]\n",
    "    )    \n",
    "    #リクエスト\n",
    "    result = chat([\n",
    "        HumanMessage(content=message.content)\n",
    "    ])\n",
    "    #\n",
    "    await cl.Message(content=f\"GPT: {result.content}\").send()    \n",
    "    # Send a response back to the user\n",
    "    # await cl.Message(\n",
    "    #     content=f\"GPT: {message.content}\",\n",
    "    # ).send()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ec15b-7f1b-4eeb-8256-89b8be67b066",
   "metadata": {},
   "source": [
    "# OpenAIのAPIを使う場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f9c70-24ce-4fec-93be-3727e71c9118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import chainlit as cl\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "# Instrument the OpenAI client\n",
    "cl.instrument_openai()\n",
    "\n",
    "settings = {\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"temperature\": 1,\n",
    "    # ... more settings\n",
    "}\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    response = await client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"content\": \"You are a helpful bot, you always reply in Japanes\",\n",
    "                \"role\": \"system\"\n",
    "            },\n",
    "            {\n",
    "                \"content\": message.content,\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "        ],\n",
    "        **settings\n",
    "    )\n",
    "    await cl.Message(content=response.choices[0].message.content).send()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ebab3-40e5-48c5-96ff-7a9b9f754557",
   "metadata": {},
   "source": [
    "## LangChainを使ったやり方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f254f1d-19ff-40de-9e2d-e5a7b34de54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#逐次表示されるチャット\n",
    "#from langchain_openai import ChatOpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "\n",
    "import chainlit as cl\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    model = ChatOpenAI(streaming=True)\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You're a very knowledgeable historian who provides accurate and eloquent answers to historical questions.\",\n",
    "            ),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "    runnable = prompt | model | StrOutputParser()\n",
    "    cl.user_session.set(\"runnable\", runnable)\n",
    "\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    runnable = cl.user_session.get(\"runnable\")  # type: Runnable\n",
    "\n",
    "    msg = cl.Message(content=\"\")\n",
    "\n",
    "    async for chunk in runnable.astream(\n",
    "        {\"question\": message.content},\n",
    "        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\n",
    "    ):\n",
    "        await msg.stream_token(chunk)\n",
    "\n",
    "    await msg.send()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358f780-b27c-4f98-900b-537ef7aecc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Deprecated] With Legacy Chain Interface\n",
    "#逐次表示されないバージョン\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "import chainlit as cl\n",
    "\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    model = ChatOpenAI(streaming=True)\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You're a very knowledgeable historian who provides accurate and eloquent answers to historical questions.\",\n",
    "            ),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "    chain = LLMChain(llm=model, prompt=prompt, output_parser=StrOutputParser())\n",
    "\n",
    "    cl.user_session.set(\"chain\", chain)\n",
    "\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    chain = cl.user_session.get(\"chain\")  # type: LLMChain\n",
    "\n",
    "    res = await chain.arun(\n",
    "        question=message.content, callbacks=[cl.LangchainCallbackHandler()]\n",
    "    )\n",
    "\n",
    "    await cl.Message(content=res).send()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa423189-b9dc-4875-8072-81f354f0c5ed",
   "metadata": {},
   "source": [
    "## 自主研究"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1378e40f-5536-4551-9b40-318cd60d15b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChatGPT 4oの場合\n",
    "#逐次表示されるチャット\n",
    "#from langchain_openai import ChatOpenAI\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "\n",
    "import chainlit as cl\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    await cl.Message(content=\"私はGPT-4oです、何でも質問してください。\").send()\n",
    "    model = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        streaming=True\n",
    "    )\n",
    "    #\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"あなたは優秀なアシスタントです。\",\n",
    "            ),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "    #\n",
    "    runnable = prompt | model | StrOutputParser()\n",
    "    #\n",
    "    cl.user_session.set(\"runnable\", runnable)\n",
    "\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    runnable = cl.user_session.get(\"runnable\")  # type: Runnable\n",
    "\n",
    "    msg = cl.Message(content=\"\")\n",
    "\n",
    "    async for chunk in runnable.astream(\n",
    "        {\"question\": message.content},\n",
    "        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\n",
    "    ):\n",
    "        await msg.stream_token(chunk)\n",
    "\n",
    "    await msg.send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd29aa0e-59b6-48c9-b6e2-a99baa0a9a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geminiの場合\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "\n",
    "import chainlit as cl\n",
    "\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    await cl.Message(content=\"私はGemini-1.5-Flashです、何でも質問してください。\").send()\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash-latest\",\n",
    "        streaming=True\n",
    "    )\n",
    "    #\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"あなたは優秀なアシスタントです。\",\n",
    "            ),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "    #\n",
    "    runnable = prompt | model | StrOutputParser()\n",
    "    #\n",
    "    cl.user_session.set(\"runnable\", runnable)\n",
    "\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    runnable = cl.user_session.get(\"runnable\")  # type: Runnable\n",
    "\n",
    "    msg = cl.Message(content=\"\")\n",
    "\n",
    "    async for chunk in runnable.astream(\n",
    "        {\"question\": message.content},\n",
    "        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\n",
    "    ):\n",
    "        await msg.stream_token(chunk)\n",
    "\n",
    "    await msg.send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5d08492-33c8-4e5b-ae24-fa53534f3e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geminiの場合\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "\n",
    "import chainlit as cl\n",
    "\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    await cl.Message(content=\"私はgemini-1.5-proです、何でも質問してください。\").send()\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-pro-latest\",\n",
    "        streaming=True\n",
    "    )\n",
    "    #\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"あなたは優秀なアシスタントです。\",\n",
    "            ),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "    #\n",
    "    runnable = prompt | model | StrOutputParser()\n",
    "    #\n",
    "    cl.user_session.set(\"runnable\", runnable)\n",
    "\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    runnable = cl.user_session.get(\"runnable\")  # type: Runnable\n",
    "\n",
    "    msg = cl.Message(content=\"\")\n",
    "\n",
    "    async for chunk in runnable.astream(\n",
    "        {\"question\": message.content},\n",
    "        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\n",
    "    ):\n",
    "        await msg.stream_token(chunk)\n",
    "\n",
    "    await msg.send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf0db28-6b8f-49d3-97ac-04e493f1725f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5db0741-fd1e-4715-92e3-864491261896",
   "metadata": {},
   "source": [
    "## 公式ドキュメントより"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f3cd11-738f-4100-ae27-1a11bd385c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " マグロは常に活動している魚です。マグロは水温を調節するために、絶えず動き続けています。マグロは、泳いでいるときに筋肉が熱を発生させ、その熱を体内に保持します。そのため、マグロは長時間休むことはできません。マグロは、短い時間だけ休むために、水中に潜ったり、水面近くで漂ったりします。 \n",
      "\n",
      "マグロは、獲物を追いかけるためにも、常に活動する必要があります。マグロは、速い速度で泳ぎ、遠くまで移動することができます。そのため、マグロは、休む時間を最小限に抑えなければなりません。 \n",
      "\n",
      "マグロは、水温の変化や餌の量によって、活動のレベルが異なります。水温が低いときや餌が少ないときは、マグロは活動レベルが低下します。しかし、マグロは、常に活動している魚であり、休む時間は非常に短いです。\n",
      "\n",
      "[parts {\n",
      "  text: \"マグロはいつ休む？\"\n",
      "}\n",
      "role: \"user\"\n",
      ", parts {\n",
      "  text: \" マグロは常に活動している魚です。マグロは水温を調節するために、絶えず動き続けています。マグロは、泳いでいるときに筋肉が熱を発生させ、その熱を体内に保持します。そのため、マグロは長時間休むことはできません。マグロは、短い時間だけ休むために、水中に潜ったり、水面近くで漂ったりします。 \\n\\nマグロは、獲物を追いかけるためにも、常に活動する必要があります。マグロは、速い速度で泳ぎ、遠くまで移動することができます。そのため、マグロは、休む時間を最小限に抑えなければなりません。 \\n\\nマグロは、水温の変化や餌の量によって、活動のレベルが異なります。水温が低いときや餌が少ないときは、マグロは活動レベルが低下します。しかし、マグロは、常に活動している魚であり、休む時間は非常に短いです。\\n\"\n",
      "}\n",
      "role: \"model\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Create the model\n",
    "# See https://ai.google.dev/api/python/google/generativeai/GenerativeModel\n",
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 64,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "safety_settings = [\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "]\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    #model_name=\"gemini-pro\", \n",
    "  #model_name=\"gemini-1.5-pro-latest\", \n",
    "    model_name=\"gemini-1.5-flash-latest\",\n",
    "  safety_settings=safety_settings,\n",
    "  generation_config=generation_config,\n",
    ")\n",
    "\n",
    "chat_session = model.start_chat(\n",
    "  history=[\n",
    "  ]\n",
    ")\n",
    "\n",
    "response = chat_session.send_message(\"マグロはいつ休む？\")\n",
    "\n",
    "print(response.text)\n",
    "print(chat_session.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12235f00-4ce0-4da8-884e-459c52996076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba9704-2f6e-4a5a-bcca-b561e0f5ff11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
