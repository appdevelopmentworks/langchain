{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "058afcc8-ab28-488d-af4f-85ecfeeda1e2",
   "metadata": {},
   "source": [
    "## Chainlitの練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dcf564-c080-4469-9f1b-c176e86b693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#オウム返し\n",
    "import chainlit as cl\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message: cl.Message):\n",
    "    # Your custom logic goes here...\n",
    "\n",
    "    # Send a response back to the user\n",
    "    await cl.Message(\n",
    "        content=f\"Received: {message.content}\",\n",
    "    ).send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3716da43-67fe-4cb9-aea7-a637d7200a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainlit as cl\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "#モデルを設定\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "#バッファーを初期化\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message: cl.Message):\n",
    "    # Your custom logic goes here...\n",
    "    chat = ChatOpenAI(\n",
    "        #ストリーミングモード指定\n",
    "        streaming=True,\n",
    "        callbacks=[\n",
    "            StreamingStdOutCallbackHandler()\n",
    "        ]\n",
    "    )    \n",
    "    #リクエスト\n",
    "    result = chat([\n",
    "        HumanMessage(content=message.content)\n",
    "    ])\n",
    "    #\n",
    "    await cl.Message(content=f\"GPT: {result.content}\").send()    \n",
    "    # Send a response back to the user\n",
    "    # await cl.Message(\n",
    "    #     content=f\"GPT: {message.content}\",\n",
    "    # ).send()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ec15b-7f1b-4eeb-8256-89b8be67b066",
   "metadata": {},
   "source": [
    "# OpenAIのAPIを使う場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f9c70-24ce-4fec-93be-3727e71c9118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import chainlit as cl\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "# Instrument the OpenAI client\n",
    "cl.instrument_openai()\n",
    "\n",
    "settings = {\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"temperature\": 1,\n",
    "    # ... more settings\n",
    "}\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    response = await client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"content\": \"You are a helpful bot, you always reply in Japanes\",\n",
    "                \"role\": \"system\"\n",
    "            },\n",
    "            {\n",
    "                \"content\": message.content,\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "        ],\n",
    "        **settings\n",
    "    )\n",
    "    await cl.Message(content=response.choices[0].message.content).send()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ebab3-40e5-48c5-96ff-7a9b9f754557",
   "metadata": {},
   "source": [
    "## LangChainを使ったやり方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f254f1d-19ff-40de-9e2d-e5a7b34de54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#逐次表示されるチャット\n",
    "#from langchain_openai import ChatOpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "\n",
    "import chainlit as cl\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    model = ChatOpenAI(streaming=True)\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You're a very knowledgeable historian who provides accurate and eloquent answers to historical questions.\",\n",
    "            ),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "    runnable = prompt | model | StrOutputParser()\n",
    "    cl.user_session.set(\"runnable\", runnable)\n",
    "\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    runnable = cl.user_session.get(\"runnable\")  # type: Runnable\n",
    "\n",
    "    msg = cl.Message(content=\"\")\n",
    "\n",
    "    async for chunk in runnable.astream(\n",
    "        {\"question\": message.content},\n",
    "        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\n",
    "    ):\n",
    "        await msg.stream_token(chunk)\n",
    "\n",
    "    await msg.send()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358f780-b27c-4f98-900b-537ef7aecc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Deprecated] With Legacy Chain Interface\n",
    "#逐次表示されないバージョン\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "import chainlit as cl\n",
    "\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    model = ChatOpenAI(streaming=True)\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You're a very knowledgeable historian who provides accurate and eloquent answers to historical questions.\",\n",
    "            ),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "    chain = LLMChain(llm=model, prompt=prompt, output_parser=StrOutputParser())\n",
    "\n",
    "    cl.user_session.set(\"chain\", chain)\n",
    "\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    chain = cl.user_session.get(\"chain\")  # type: LLMChain\n",
    "\n",
    "    res = await chain.arun(\n",
    "        question=message.content, callbacks=[cl.LangchainCallbackHandler()]\n",
    "    )\n",
    "\n",
    "    await cl.Message(content=res).send()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa423189-b9dc-4875-8072-81f354f0c5ed",
   "metadata": {},
   "source": [
    "## 自主研究"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1378e40f-5536-4551-9b40-318cd60d15b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#逐次表示されるチャット\n",
    "#from langchain_openai import ChatOpenAI\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "\n",
    "import chainlit as cl\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    model = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        streaming=True\n",
    "    )\n",
    "    #\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"あなたはウォーレン・バフェット並みに優秀な投資家です、投資のアドバイスをしてください。\",\n",
    "            ),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "    #\n",
    "    runnable = prompt | model | StrOutputParser()\n",
    "    #\n",
    "    cl.user_session.set(\"runnable\", runnable)\n",
    "\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    runnable = cl.user_session.get(\"runnable\")  # type: Runnable\n",
    "\n",
    "    msg = cl.Message(content=\"\")\n",
    "\n",
    "    async for chunk in runnable.astream(\n",
    "        {\"question\": message.content},\n",
    "        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\n",
    "    ):\n",
    "        await msg.stream_token(chunk)\n",
    "\n",
    "    await msg.send()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
