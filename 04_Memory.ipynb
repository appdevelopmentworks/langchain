{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b98ced4-c180-4ef3-a49b-60992c684870",
   "metadata": {},
   "source": [
    "## 言語モデルにおける会話"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9567d9f6-45f8-4e96-8ddf-0cb3d1527957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.schema import AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "178a90a5-817e-4ec8-b5e2-f4b180490950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "だし巻き卵を作るために必要な基本的な食材は以下の通りです：\n",
      "\n",
      "1. 卵：4個（一般的なレシピの場合）\n",
      "2. だし：大さじ2（だし汁、または市販のだしパウダーを水で溶かしたもの）\n",
      "3. 砂糖：小さじ1（お好みで量を調整）\n",
      "4. 醤油：小さじ1（お好みで量を調整）\n",
      "5. 塩：少々（お好みで量を調整）\n",
      "6. 油：適量（卵焼き器に塗るため）\n",
      "\n",
      "これらの食材を用意すれば、だし巻き卵を作ることができます。お好みに応じて、みりんを加える場合もあります。\n",
      "\n",
      "作り方は以下の通りです：\n",
      "\n",
      "1. ボウルに卵を割り入れ、よく溶きます。\n",
      "2. 溶き卵にだし、砂糖、醤油、塩を加えてよく混ぜます。\n",
      "3. 卵焼き器（またはフライパン）に油を薄く塗り、熱します。\n",
      "4. 卵液を薄く広げ、固まり始めたら端から巻いていきます。\n",
      "5. 卵液を追加し、再度薄く広げ、固まり始めたら巻いていきます。これを何度か繰り返します。\n",
      "6. 形を整えながら焼き上げます。\n",
      "\n",
      "焼き上がったら、少し冷ましてから切り分けてお召し上がりください。\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    ")\n",
    "#\n",
    "result = chat(\n",
    "    [\n",
    "        HumanMessage(content=\"だし巻き卵を作るのに必要な食材を教えて\")\n",
    "    ]\n",
    ")\n",
    "#\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95ab29c5-2ffe-42cf-8785-3799b0ac29dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is the translation:\n",
      "\n",
      "To make \"dashimaki tamago\" (Japanese rolled omelette with dashi), you need the following ingredients:\n",
      "1. Eggs\n",
      "2. Dashi (Japanese soup stock)\n",
      "3. Sugar\n",
      "4. Salt\n",
      "5. Soy sauce\n",
      "6. Oil\n"
     ]
    }
   ],
   "source": [
    "result = chat(\n",
    "    [\n",
    "        HumanMessage(content=\"だし巻き卵を作るのに必要な食材を教えて\"),\n",
    "        AIMessage(content=\"\"\"だし巻き卵を作るために必要な食材は以下の通りです。\n",
    "                                1. 卵\n",
    "                                2. だし（出汁）\n",
    "                                3. 砂糖\n",
    "                                4. 塩\n",
    "                                5. しょうゆ\n",
    "                                6. 油\n",
    "                                \"\"\"\n",
    "                 ),\n",
    "        HumanMessage(content=\"前回の回答を英語に翻訳して\")\n",
    "        \n",
    "    ]\n",
    ")\n",
    "#\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2b3db5-c021-4c07-b0eb-94a853ce88bd",
   "metadata": {},
   "source": [
    "## 必要に応じた返答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d2dd469-184b-492d-832e-6d288739f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f614f365-9d67-4b2d-abe5-91fc1c21ba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [HumanMessage(content='こんにちは！'), AIMessage(content='こんにちは！お元気ですか？栄養の相談があればお気軽に。'), HumanMessage(content='今日は胃がもたれてます。'), AIMessage(content='それなら、居酒屋川崎のだし巻き卵はいかがでしょう？')]}\n"
     ]
    }
   ],
   "source": [
    "#バッファーを初期化\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "#メモリーにメッセージを追加\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\":\"こんにちは！\"\n",
    "    },\n",
    "    {\n",
    "        \"output\":\"こんにちは！お元気ですか？栄養の相談があればお気軽に。\"\n",
    "    }\n",
    ")\n",
    "\n",
    "#メモリーにメッセージを追加\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\":\"今日は胃がもたれてます。\"\n",
    "    },\n",
    "    {\n",
    "        \"output\":\"それなら、居酒屋川崎のだし巻き卵はいかがでしょう？\"\n",
    "    }\n",
    ")\n",
    "\n",
    "#メモリー内容を確認\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d01f75a-eed4-4fb7-95f5-ff5a7e7d90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chainlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f3d7192-a31d-4d3e-9014-d70dabd0e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d747071-6254-4482-a2fc-04cfb97d45df",
   "metadata": {},
   "source": [
    "## 会話履歴を保存し呼び出せるConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccdc5d44-a734-4f72-a224-bfb5131bb3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainlit as cl\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "#モデルを設定\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "#ConversationBufferMemoryを初期化\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    await cl.Message(content=\"私は会話の文脈を考慮したうえで返答ができるチャットボットです。\").send()\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    #会話履歴のメモリーをロード\n",
    "    memory_message_result = memory.load_memory_variables({})\n",
    "    #辞書型データから会話の部分だけを配列に格納\n",
    "    messages = memory_message_result['history']\n",
    "    #今、入力されたメッセージを追加\n",
    "    messages.append(HumanMessage(content=message.content))\n",
    "    #会話履歴をそのまま渡す\n",
    "    result = chat(\n",
    "        messages\n",
    "    )\n",
    "    #メモリーにユーザー入力とLLMのレスを辞書型で保存\n",
    "    memory.save_context(\n",
    "        {\n",
    "            \"input\": message.content,\n",
    "        },\n",
    "        {\n",
    "            \"output\":result.content,\n",
    "        }\n",
    "    )\n",
    "    #LLMの返答を返す\n",
    "    await cl.Message(content=result.content).send()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed63aa34-d1c9-46f2-bb00-c69c34571a90",
   "metadata": {},
   "source": [
    "## 会話履歴を考慮したチャットをConversationChainを使って簡単に"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fefea88-3d43-4628-883e-91895462db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import chainlit as cl\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage\n",
    "#\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "#モデルを設定\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "#会話バッファーを初期化\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "#ConversationChainをインスタンス化\n",
    "chain = ConversationChain(\n",
    "    memory=memory,\n",
    "    llm=chat,\n",
    ")\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    await cl.Message(content=\"私は会話の文脈を考慮したうえで返答ができるチャットボットです。\").send()\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    result = chain(\n",
    "        message.content\n",
    "    )\n",
    "    #LLMの応答を返す\n",
    "    await cl.Message(content=result[\"response\"]).send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39bc3e-2092-45e4-b119-9439639e4648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#上のコードをGeminiで\n",
    "import chainlit as cl\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage\n",
    "#\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "#モデルを設定\n",
    "chat = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash-latest\"\n",
    ")\n",
    "\n",
    "#会話バッファーを初期化\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "#ConversationChainをインスタンス化\n",
    "chain = ConversationChain(\n",
    "    memory=memory,\n",
    "    llm=chat,\n",
    ")\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    await cl.Message(content=\"私は会話の文脈を考慮したうえで返答ができるチャットボットです。\").send()\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    result = chain(\n",
    "        message.content\n",
    "    )\n",
    "    #LLMの応答を返す\n",
    "    await cl.Message(content=result[\"response\"]).send()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b5f1b-1ccf-4d90-9335-fbbc85fdc279",
   "metadata": {},
   "source": [
    "## 履歴をデータベースに保存\n",
    "\n",
    "1.以下のURLにアクセス\n",
    "https://upstash.com/\n",
    "\n",
    "2.Create Database\n",
    "- Name \n",
    "- Type Regional\n",
    "- Region Japan\n",
    "\n",
    "3.Key\n",
    "\n",
    "4.Pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69d50d06-7efa-464d-901d-60eb0d5fc541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: redis in c:\\users\\hartm\\anaconda3\\lib\\site-packages (5.0.4)\n"
     ]
    }
   ],
   "source": [
    "#redisのインストール\n",
    "!pip install redis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604ccca-1fff-4db7-98c8-e5dd80087d49",
   "metadata": {},
   "source": [
    "## Redisで会話を永続化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a556117-6f5a-4398-aa21-60c685daa8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-22 16:09:58 - Loaded .env file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hartm\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "#RedisChatMessageHistoryでDBに接続できず\n",
    "import os\n",
    "import chainlit as cl\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import RedisChatMessageHistory  #← RedisChatMessageHistoryを追加\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "#← RedisChatMessageHistoryを初期化\n",
    "history = RedisChatMessageHistory(  \n",
    "    session_id=\"chat_history\",\n",
    "    url=os.environ[\"REDIS_URL\"]\n",
    ")\n",
    "\n",
    "# history = RedisChatMessageHistory(session_id=\"chat_history\",\n",
    "#                                   url=os.environ[\"UPSTASH_REDIS_REST_URL\"],\n",
    "#                                   key_prefix=os.environ[\"UPSTASH_REDIS_REST_TOKEN\"])\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True,\n",
    "    chat_memory=history,\n",
    ")\n",
    "\n",
    "chain = ConversationChain(\n",
    "    memory=memory,\n",
    "    llm=chat,\n",
    ")\n",
    "\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    await cl.Message(content=\"私は会話の文脈を考慮した返答をできるチャットボットです。メッセージを入力してください。\").send()\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: str):\n",
    "    #\n",
    "    result=chain(message.content)\n",
    "    #\n",
    "    await cl.Message(content=result[\"response\"]).send()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74777214-3a02-4721-b335-3a4b20fe4cee",
   "metadata": {},
   "source": [
    "## 複数の会話履歴を持てるチャットボット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b3eb51-09d1-4adc-969e-e03e33e9f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RedisChatMessageHistoryへの保存がどうも上手くいかない\n",
    "import os\n",
    "import chainlit as cl\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory, RedisChatMessageHistory\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    thread_id = None\n",
    "    #スレッドIDが入力されるまで繰り返す\n",
    "    while not thread_id:\n",
    "        #AskUserMessageを使ってスレッドIDを入力\n",
    "        res = await cl.AskUserMessage(content=\"私は会話の文脈を考慮した返答ができるチャットボットです。スレッドIDを入力してください。\", \n",
    "                                      timeout=600).send()\n",
    "        if res:\n",
    "            thread_id = res['content']\n",
    "    #新しくチャットが始まるたびに初期化するようにon_chat_startに移動\n",
    "    history = RedisChatMessageHistory(  \n",
    "        session_id=thread_id,\n",
    "        url=os.environ.get(\"REDIS_URL\"),\n",
    "    )\n",
    "    \n",
    "    #新しくチャットが始まるたびに初期化するようにon_chat_startに移動\n",
    "    memory = ConversationBufferMemory( \n",
    "        return_messages=True,\n",
    "        chat_memory=history,\n",
    "    )\n",
    "    #新しくチャットが始まるたびに初期化するようにon_chat_startに移動\n",
    "    chain = ConversationChain( \n",
    "        memory=memory,\n",
    "        llm=chat,\n",
    "    )\n",
    "    \n",
    "    #メモリの内容を取得\n",
    "    memory_message_result = chain.memory.load_memory_variables({})\n",
    "\n",
    "    messages = memory_message_result['history']\n",
    "\n",
    "    for message in messages:\n",
    "        #ユーザーからのメッセージかどうかを判定\n",
    "        if isinstance(message, HumanMessage): \n",
    "            #ユーザーからのメッセージの場合はauthorUserを指定して送信\n",
    "            await cl.Message( \n",
    "                author=\"User\",\n",
    "                content=f\"{message.content}\",\n",
    "            ).send()\n",
    "        else:\n",
    "            #AIからのメッセージの場合はChatBotを指定して送信\n",
    "            await cl.Message(\n",
    "                author=\"ChatBot\",\n",
    "                content=f\"{message.content}\",\n",
    "            ).send()\n",
    "    #履歴をセッションに保存\n",
    "    cl.user_session.set(\"chain\", chain)\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    #セッションから履歴を取得\n",
    "    chain = cl.user_session.get(\"chain\")\n",
    "\n",
    "    result = chain(message.content)\n",
    "\n",
    "    await cl.Message(content=result[\"response\"]).send()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0db18f-c2c7-47f6-8663-34cee78235f1",
   "metadata": {},
   "source": [
    "## 非常に長い会話に対応"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68815d4c-d9b5-4f9a-b787-9683366286c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "chat=ChatOpenAI()\n",
    "\n",
    "result=chat([\n",
    "    HumanMessage(content=\"茶碗蒸しの作り方教えて\"),\n",
    "    AIMessage(content=\"{ChatModelからの返答である茶碗蒸しの作り方}\"),\n",
    "    HumanMessage(content=\"だし巻き玉子の作り方教えて\"),\n",
    "    AIMessage(content=\"{ChatModelからの返答であるだし巻き卵の作り方}\"),\n",
    "    HumanMessage(content=\"チャーハンの作り方教えて\"),\n",
    "])\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab2eff1-73bd-4b45-b381-1b513d7413d5",
   "metadata": {},
   "source": [
    "### 会話履歴の保存数を指定する場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ebdf8df-cc2c-4c54-8474-a00750b5b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainlit as cl\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "#3往復分のメッセージを記憶する\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "chain = ConversationChain(\n",
    "    memory=memory,\n",
    "    llm=chat,\n",
    ")\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    await cl.Message(content=\"私は会話の文脈を考慮した返答ができるチャットボットです。メッセージを入力してください。\").send()\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    #保存されているメッセージを取得する\n",
    "    messages = chain.memory.load_memory_variables({})[\"history\"]\n",
    "    #保存されているメッセージの数を表示する\n",
    "    print(f\"保存されているメッセージの数: {len(messages)}\")\n",
    "    #保存されているメッセージを1つずつ取り出す\n",
    "    for saved_message in messages: \n",
    "        #保存されているメッセージを表示する\n",
    "        print(saved_message.content)\n",
    "\n",
    "    result = chain(message.content)\n",
    "\n",
    "    await cl.Message(content=result[\"response\"]).send()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399f5719-d582-4510-96d7-09324f16b309",
   "metadata": {},
   "source": [
    "### 会話履歴を要約してトークン数を抑える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7289a0-1a63-441d-a7c9-f8f8532c53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainlit as cl\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory \n",
    "from langchain.memory import ConversationSummaryMemory \n",
    "from langchain.chains import ConversationChain \n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationChain \n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "#ConversationSummaryMemoryを使用するように変更\n",
    "#会話履歴を要約し1つにまとめる\n",
    "memory = ConversationSummaryMemory(  \n",
    "    llm=chat,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "chain = ConversationChain(\n",
    "    memory=memory,\n",
    "    llm=chat,\n",
    ")\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    await cl.Message(content=\"私は会話の文脈を考慮した返答をできるチャットボットです。メッセージを入力してください。\").send()\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    #保存されているメッセージを取得する\n",
    "    messages = chain.memory.load_memory_variables({})[\"history\"]\n",
    "    #要約されて１つになるのを確認用として\n",
    "    print(f\"保存されているメッセージの数: {len(messages)}\")\n",
    "    #保存されているメッセージを1つずつ取り出す\n",
    "    for saved_message in messages:\n",
    "        #保存されているメッセージを表示する\n",
    "        print(saved_message.content)\n",
    "\n",
    "    result = chain(message.content)\n",
    "\n",
    "    await cl.Message(content=result[\"response\"]).send()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
